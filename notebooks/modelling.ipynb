{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58ba1fc9",
   "metadata": {},
   "source": [
    "# Predicting Employee Attrition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e538e670",
   "metadata": {},
   "source": [
    "Installing and Importing Libaries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d408cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install imbalanced-learn\n",
    "#!pip install xgboost lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3755bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif, chi2, RFE\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif, mutual_info_classif\n",
    "from scipy.stats import spearmanr, kendalltau, pearsonr\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, classification_report\n",
    "from sklearn.utils import resample\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2de109",
   "metadata": {},
   "source": [
    "Importing file (csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0304a332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (1470, 31)\n",
      "\n",
      "First few rows:\n",
      "   Age  Attrition     BusinessTravel  DailyRate              Department  \\\n",
      "0   41          1      Travel_Rarely       1102                   Sales   \n",
      "1   49          0  Travel_Frequently        279  Research & Development   \n",
      "2   37          1      Travel_Rarely       1373  Research & Development   \n",
      "3   33          0  Travel_Frequently       1392  Research & Development   \n",
      "4   27          0      Travel_Rarely        591  Research & Development   \n",
      "\n",
      "   DistanceFromHome      Education EducationField EnvironmentSatisfaction  \\\n",
      "0                 1        College  Life Sciences                  Medium   \n",
      "1                 8  Below College  Life Sciences                    High   \n",
      "2                 2        College          Other               Very High   \n",
      "3                 3         Master  Life Sciences               Very High   \n",
      "4                 2  Below College        Medical                     Low   \n",
      "\n",
      "   Gender  ...  PerformanceRating RelationshipSatisfaction StockOptionLevel  \\\n",
      "0  Female  ...          Excellent                      Low              NaN   \n",
      "1    Male  ...        Outstanding                Very High              Low   \n",
      "2    Male  ...          Excellent                   Medium              NaN   \n",
      "3  Female  ...          Excellent                     High              NaN   \n",
      "4    Male  ...          Excellent                Very High              Low   \n",
      "\n",
      "  TotalWorkingYears TrainingTimesLastYear WorkLifeBalance  YearsAtCompany  \\\n",
      "0                 8                     0             Bad               6   \n",
      "1                10                     3          Better              10   \n",
      "2                 7                     3          Better               0   \n",
      "3                 8                     3          Better               8   \n",
      "4                 6                     3          Better               2   \n",
      "\n",
      "   YearsInCurrentRole  YearsSinceLastPromotion  YearsWithCurrManager  \n",
      "0                   4                        0                     5  \n",
      "1                   7                        1                     7  \n",
      "2                   0                        0                     0  \n",
      "3                   7                        3                     0  \n",
      "4                   2                        2                     2  \n",
      "\n",
      "[5 rows x 31 columns]\n",
      "\n",
      "Column types:\n",
      "Age                          int64\n",
      "Attrition                    int64\n",
      "BusinessTravel              object\n",
      "DailyRate                    int64\n",
      "Department                  object\n",
      "DistanceFromHome             int64\n",
      "Education                   object\n",
      "EducationField              object\n",
      "EnvironmentSatisfaction     object\n",
      "Gender                      object\n",
      "HourlyRate                   int64\n",
      "JobInvolvement              object\n",
      "JobLevel                    object\n",
      "JobRole                     object\n",
      "JobSatisfaction             object\n",
      "MaritalStatus               object\n",
      "MonthlyIncome                int64\n",
      "MonthlyRate                  int64\n",
      "NumCompaniesWorked           int64\n",
      "OverTime                     int64\n",
      "PercentSalaryHike            int64\n",
      "PerformanceRating           object\n",
      "RelationshipSatisfaction    object\n",
      "StockOptionLevel            object\n",
      "TotalWorkingYears            int64\n",
      "TrainingTimesLastYear        int64\n",
      "WorkLifeBalance             object\n",
      "YearsAtCompany               int64\n",
      "YearsInCurrentRole           int64\n",
      "YearsSinceLastPromotion      int64\n",
      "YearsWithCurrManager         int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Import the treated data\n",
    "df = pd.read_csv('../dados/df.csv')\n",
    "\n",
    "# Check the data\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "print(\"\\nColumn types:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ceef63af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Age  Attrition     BusinessTravel  DailyRate              Department  \\\n",
      "0   41          1      Travel_Rarely       1102                   Sales   \n",
      "1   49          0  Travel_Frequently        279  Research & Development   \n",
      "2   37          1      Travel_Rarely       1373  Research & Development   \n",
      "3   33          0  Travel_Frequently       1392  Research & Development   \n",
      "4   27          0      Travel_Rarely        591  Research & Development   \n",
      "\n",
      "   DistanceFromHome      Education EducationField EnvironmentSatisfaction  \\\n",
      "0                 1        College  Life Sciences                  Medium   \n",
      "1                 8  Below College  Life Sciences                    High   \n",
      "2                 2        College          Other               Very High   \n",
      "3                 3         Master  Life Sciences               Very High   \n",
      "4                 2  Below College        Medical                     Low   \n",
      "\n",
      "   Gender  ...  PerformanceRating RelationshipSatisfaction StockOptionLevel  \\\n",
      "0  Female  ...          Excellent                      Low              NaN   \n",
      "1    Male  ...        Outstanding                Very High              Low   \n",
      "2    Male  ...          Excellent                   Medium              NaN   \n",
      "3  Female  ...          Excellent                     High              NaN   \n",
      "4    Male  ...          Excellent                Very High              Low   \n",
      "\n",
      "  TotalWorkingYears TrainingTimesLastYear WorkLifeBalance  YearsAtCompany  \\\n",
      "0                 8                     0             Bad               6   \n",
      "1                10                     3          Better              10   \n",
      "2                 7                     3          Better               0   \n",
      "3                 8                     3          Better               8   \n",
      "4                 6                     3          Better               2   \n",
      "\n",
      "   YearsInCurrentRole  YearsSinceLastPromotion  YearsWithCurrManager  \n",
      "0                   4                        0                     5  \n",
      "1                   7                        1                     7  \n",
      "2                   0                        0                     0  \n",
      "3                   7                        3                     0  \n",
      "4                   2                        2                     2  \n",
      "\n",
      "[5 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adbee4eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age                          int64\n",
      "Attrition                    int64\n",
      "BusinessTravel              object\n",
      "DailyRate                    int64\n",
      "Department                  object\n",
      "DistanceFromHome             int64\n",
      "Education                   object\n",
      "EducationField              object\n",
      "EnvironmentSatisfaction     object\n",
      "Gender                      object\n",
      "HourlyRate                   int64\n",
      "JobInvolvement              object\n",
      "JobLevel                    object\n",
      "JobRole                     object\n",
      "JobSatisfaction             object\n",
      "MaritalStatus               object\n",
      "MonthlyIncome                int64\n",
      "MonthlyRate                  int64\n",
      "NumCompaniesWorked           int64\n",
      "OverTime                     int64\n",
      "PercentSalaryHike            int64\n",
      "PerformanceRating           object\n",
      "RelationshipSatisfaction    object\n",
      "StockOptionLevel            object\n",
      "TotalWorkingYears            int64\n",
      "TrainingTimesLastYear        int64\n",
      "WorkLifeBalance             object\n",
      "YearsAtCompany               int64\n",
      "YearsInCurrentRole           int64\n",
      "YearsSinceLastPromotion      int64\n",
      "YearsWithCurrManager         int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Check data types\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373fbb6c",
   "metadata": {},
   "source": [
    "## 3. Feature Selection "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63813f8",
   "metadata": {},
   "source": [
    "### 3.1. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0988be",
   "metadata": {},
   "source": [
    "Feature Selection Methods Applied:\n",
    "\n",
    "- **Spearman Correlation**: Features with correlation > 0.1 AND p-value < 0.05\n",
    "- **Mutual Information**: Top 30% highest mutual information scores  \n",
    "- **ANOVA F-test**: Top 15 most statistically significant numeric features\n",
    "- **Random Forest**: Top 30% most important features from ensemble model\n",
    "\n",
    "Selection Criteria:\n",
    "\n",
    "- Features receive 1 \"vote\" for each method that selects them\n",
    "- **Final selection**: Features with â‰¥1 vote (selected by at least 1 method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a606396c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Target distribution: Attrition\n",
      "0    1233\n",
      "1     237\n",
      "Name: count, dtype: int64 (1=Yes, 0=No)\n",
      "\n",
      "ðŸ“Š Feature Categories:\n",
      "â€¢ Numerical: 14 features\n",
      "â€¢ Categorical (one-hot): 15 features\n",
      "â€¢ Binary: 1 features\n",
      "â€¢ Total features before encoding: 30\n",
      "\n",
      "âœ… Preprocessing complete!\n",
      "Original features: 30\n",
      "After one-hot encoding: 62\n",
      "One-hot features created: 47\n",
      "\n",
      "ðŸ” Sample one-hot encoded columns:\n",
      "  BusinessTravel_Travel_Frequently\n",
      "  BusinessTravel_Travel_Rarely\n",
      "  Department_Research & Development\n",
      "  Department_Sales\n",
      "  Education_Below College\n",
      "  Education_College\n",
      "  Education_Doctor\n",
      "  Education_Master\n",
      "  EducationField_Life Sciences\n",
      "  EducationField_Marketing\n",
      "\n",
      "1. Running Spearman correlation...\n",
      "  âœ“ Age: corr=-0.171\n",
      "  âœ“ MonthlyIncome: corr=-0.198\n",
      "  âœ“ TotalWorkingYears: corr=-0.199\n",
      "  âœ“ YearsAtCompany: corr=-0.190\n",
      "  âœ“ YearsInCurrentRole: corr=-0.181\n",
      "  âœ“ YearsWithCurrManager: corr=-0.175\n",
      "2. Running Mutual Information...\n",
      "  Selected 19 features via MI\n",
      "3. Running ANOVA...\n",
      "  Selected 14 features via ANOVA\n",
      "4. Running Random Forest...\n",
      "  Selected 19 features via Random Forest\n",
      "\n",
      "============================================================\n",
      "FEATURE SELECTION RESULTS\n",
      "============================================================\n",
      "                             Feature  Votes\n",
      "0                                Age      4\n",
      "4                      MonthlyIncome      4\n",
      "10                    YearsAtCompany      4\n",
      "11                YearsInCurrentRole      4\n",
      "13              YearsWithCurrManager      4\n",
      "8                  TotalWorkingYears      4\n",
      "5                        MonthlyRate      2\n",
      "3                         HourlyRate      2\n",
      "2                   DistanceFromHome      2\n",
      "1                          DailyRate      2\n",
      "9              TrainingTimesLastYear      2\n",
      "7                  PercentSalaryHike      2\n",
      "6                 NumCompaniesWorked      2\n",
      "12           YearsSinceLastPromotion      2\n",
      "14                          OverTime      2\n",
      "58              StockOptionLevel_nan      2\n",
      "51              MaritalStatus_Single      2\n",
      "15  BusinessTravel_Travel_Frequently      1\n",
      "28       EnvironmentSatisfaction_Low      1\n",
      "36           JobLevel_Lab Technician      1\n",
      "\n",
      "âœ… SELECTED features (â‰¥2 votes): 29\n",
      "âŒ REMOVED features (<2 votes): 33\n",
      "\n",
      "Reduced dataset shape: (1470, 29)\n",
      "\n",
      "ðŸ† TOP SELECTED FEATURES:\n",
      "  Age: 4 votes\n",
      "  MonthlyIncome: 4 votes\n",
      "  YearsAtCompany: 4 votes\n",
      "  YearsInCurrentRole: 4 votes\n",
      "  YearsWithCurrManager: 4 votes\n",
      "  TotalWorkingYears: 4 votes\n",
      "  MonthlyRate: 2 votes\n",
      "  HourlyRate: 2 votes\n",
      "  DistanceFromHome: 2 votes\n",
      "  DailyRate: 2 votes\n",
      "  TrainingTimesLastYear: 2 votes\n",
      "  PercentSalaryHike: 2 votes\n",
      "  NumCompaniesWorked: 2 votes\n",
      "  YearsSinceLastPromotion: 2 votes\n",
      "  OverTime: 2 votes\n"
     ]
    }
   ],
   "source": [
    "# 1. Prepare X and y (encode binary targets first)\n",
    "X = df.drop('Attrition', axis=1)\n",
    "y = df['Attrition'] \n",
    "\n",
    "\n",
    "print(f\"âœ… Target distribution: {y.value_counts()} (1=Yes, 0=No)\")\n",
    "\n",
    "# 2. Feature Groups - ALL categorical variables will be one-hot encoded\n",
    "numeric_features = [\n",
    "    \"Age\", \"DailyRate\", \"DistanceFromHome\", \"HourlyRate\", \"MonthlyIncome\",\n",
    "    \"MonthlyRate\", \"NumCompaniesWorked\", \"PercentSalaryHike\", \"TotalWorkingYears\",\n",
    "    \"TrainingTimesLastYear\", \"YearsAtCompany\", \"YearsInCurrentRole\",\n",
    "    \"YearsSinceLastPromotion\", \"YearsWithCurrManager\"\n",
    "]\n",
    "\n",
    "# ALL categorical features (both nominal and ordinal) for one-hot encoding\n",
    "categorical_features = [\n",
    "    \"BusinessTravel\", \"Department\", \"Education\", \"EducationField\", \n",
    "    \"EnvironmentSatisfaction\", \"Gender\", \"JobInvolvement\", \"JobLevel\", \n",
    "    \"JobRole\", \"JobSatisfaction\", \"MaritalStatus\", \"PerformanceRating\", \n",
    "    \"RelationshipSatisfaction\", \"StockOptionLevel\", \"WorkLifeBalance\"\n",
    "]\n",
    "\n",
    "binary_features = [\"OverTime\"]\n",
    "\n",
    "print(f\"\\nðŸ“Š Feature Categories:\")\n",
    "print(f\"â€¢ Numerical: {len(numeric_features)} features\")\n",
    "print(f\"â€¢ Categorical (one-hot): {len(categorical_features)} features\") \n",
    "print(f\"â€¢ Binary: {len(binary_features)} features\")\n",
    "print(f\"â€¢ Total features before encoding: {len(X.columns)}\")\n",
    "\n",
    "# 3. ColumnTransformer - One-hot encode ALL categorical features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", \"passthrough\", numeric_features + binary_features),\n",
    "        (\"cat\", OneHotEncoder(drop=\"first\", sparse_output=False), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Apply preprocessing\n",
    "X_encoded = preprocessor.fit_transform(X)\n",
    "\n",
    "# 4. Extract Feature Names\n",
    "numeric_binary_features = numeric_features + binary_features\n",
    "\n",
    "# Get one-hot encoded names\n",
    "ohe = preprocessor.named_transformers_['cat']\n",
    "ohe_feature_names = []\n",
    "for col, cats in zip(categorical_features, ohe.categories_):\n",
    "    for cat in cats[1:]:  # drop='first' â†’ skip first category\n",
    "        ohe_feature_names.append(f\"{col}_{cat}\")\n",
    "\n",
    "# Combine all feature names\n",
    "feature_names = numeric_binary_features + ohe_feature_names\n",
    "\n",
    "# Create final encoded DataFrame\n",
    "X_encoded_df = pd.DataFrame(X_encoded, columns=feature_names, index=X.index)\n",
    "\n",
    "print(f\"\\nâœ… Preprocessing complete!\")\n",
    "print(f\"Original features: {len(X.columns)}\")\n",
    "print(f\"After one-hot encoding: {len(X_encoded_df.columns)}\")\n",
    "print(f\"One-hot features created: {len(ohe_feature_names)}\")\n",
    "\n",
    "# Show sample of one-hot encoded columns\n",
    "print(f\"\\nðŸ” Sample one-hot encoded columns:\")\n",
    "for col in ohe_feature_names[:10]:\n",
    "    print(f\"  {col}\")\n",
    "\n",
    "# 5. Feature Voting System\n",
    "feature_votes = {feature: 0 for feature in X_encoded_df.columns}\n",
    "\n",
    "# =======================\n",
    "# METHOD 1: Spearman (numeric features only)\n",
    "# =======================\n",
    "print(\"\\n1. Running Spearman correlation...\")\n",
    "for feature in numeric_features:\n",
    "    try:\n",
    "        corr, p_value = spearmanr(X_encoded_df[feature], y)\n",
    "        if abs(corr) > 0.1 and p_value < 0.05:\n",
    "            feature_votes[feature] += 1\n",
    "            print(f\"  âœ“ {feature}: corr={corr:.3f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  âœ— {feature}: {e}\")\n",
    "\n",
    "# =======================\n",
    "# METHOD 2: Mutual Information\n",
    "# =======================\n",
    "print(\"2. Running Mutual Information...\")\n",
    "mi_scores = mutual_info_classif(X_encoded_df, y, random_state=42)\n",
    "mi_threshold = np.percentile(mi_scores, 70)\n",
    "mi_selected_count = 0\n",
    "for feature, score in zip(X_encoded_df.columns, mi_scores):\n",
    "    if score > mi_threshold:\n",
    "        feature_votes[feature] += 1\n",
    "        mi_selected_count += 1\n",
    "print(f\"  Selected {mi_selected_count} features via MI\")\n",
    "\n",
    "# =======================\n",
    "# METHOD 3: ANOVA (numeric features)\n",
    "# =======================\n",
    "print(\"3. Running ANOVA...\")\n",
    "if len(numeric_features) > 0:\n",
    "    selector_anova = SelectKBest(f_classif, k=min(15, len(numeric_features)))\n",
    "    selector_anova.fit(X_encoded_df[numeric_features], y)\n",
    "    anova_selected = np.array(numeric_features)[selector_anova.get_support()]\n",
    "    for feature in anova_selected:\n",
    "        feature_votes[feature] += 1\n",
    "    print(f\"  Selected {len(anova_selected)} features via ANOVA\")\n",
    "\n",
    "# =======================\n",
    "# METHOD 4: Random Forest\n",
    "# =======================\n",
    "print(\"4. Running Random Forest...\")\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_encoded_df, y)\n",
    "\n",
    "threshold = np.percentile(rf.feature_importances_, 70)\n",
    "rf_selected_count = 0\n",
    "for feature, importance in zip(X_encoded_df.columns, rf.feature_importances_):\n",
    "    if importance > threshold:\n",
    "        feature_votes[feature] += 1\n",
    "        rf_selected_count += 1\n",
    "print(f\"  Selected {rf_selected_count} features via Random Forest\")\n",
    "\n",
    "# ======================================================\n",
    "# 6. Results\n",
    "# ======================================================\n",
    "results = pd.DataFrame({\n",
    "    \"Feature\": list(feature_votes.keys()),\n",
    "    \"Votes\": list(feature_votes.values())\n",
    "}).sort_values(\"Votes\", ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FEATURE SELECTION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(results.head(20))\n",
    "\n",
    "# Select features with â‰¥ 1 vote\n",
    "selected_features = results[results[\"Votes\"] >= 1][\"Feature\"].tolist()\n",
    "removed_features = results[results[\"Votes\"] < 1][\"Feature\"].tolist()\n",
    "\n",
    "print(f\"\\nâœ… SELECTED features (â‰¥2 votes): {len(selected_features)}\")\n",
    "print(f\"âŒ REMOVED features (<2 votes): {len(removed_features)}\")\n",
    "\n",
    "X_reduced = X_encoded_df[selected_features]\n",
    "print(f\"\\nReduced dataset shape: {X_reduced.shape}\")\n",
    "\n",
    "# Show top selected features\n",
    "print(f\"\\nðŸ† TOP SELECTED FEATURES:\")\n",
    "top_features = results[results[\"Votes\"] >= 2].head(15)\n",
    "for idx, row in top_features.iterrows():\n",
    "    print(f\"  {row['Feature']}: {row['Votes']} votes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b30d9260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chi2 selected 14 features\n",
      "FEATURE SELECTION RESULTS TABLE (After One-Hot Encoding)\n",
      "====================================================================================================\n",
      "Feature                             Spearman  Chi2  ANOVA  RF   Votes  Selected\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Age                                 YES       NO    YES    YES  3      YES     \n",
      "DailyRate                           NO        NO    YES    YES  2      YES     \n",
      "DistanceFromHome                    NO        NO    YES    YES  2      YES     \n",
      "HourlyRate                          NO        NO    YES    YES  2      YES     \n",
      "MonthlyIncome                       YES       NO    YES    YES  3      YES     \n",
      "MonthlyRate                         NO        NO    YES    YES  2      YES     \n",
      "NumCompaniesWorked                  NO        NO    YES    YES  2      YES     \n",
      "PercentSalaryHike                   NO        NO    YES    YES  2      YES     \n",
      "TotalWorkingYears                   YES       NO    YES    YES  3      YES     \n",
      "TrainingTimesLastYear               NO        NO    YES    YES  2      YES     \n",
      "YearsAtCompany                      YES       NO    YES    YES  3      YES     \n",
      "YearsInCurrentRole                  YES       NO    YES    YES  3      YES     \n",
      "YearsSinceLastPromotion             NO        NO    YES    YES  2      YES     \n",
      "YearsWithCurrManager                YES       NO    YES    YES  3      YES     \n",
      "OverTime                            N/A       NO    N/A    YES  1      NO      \n",
      "BusinessTravel_Travel_Frequently    N/A       YES   N/A    YES  2      YES     \n",
      "BusinessTravel_Travel_Rarely        N/A       NO    N/A    NO   0      NO      \n",
      "Department_Research & Development   N/A       YES   N/A    NO   1      NO      \n",
      "Department_Sales                    N/A       NO    N/A    NO   0      NO      \n",
      "Education_Below College             N/A       NO    N/A    NO   0      NO      \n",
      "Education_College                   N/A       NO    N/A    NO   0      NO      \n",
      "Education_Doctor                    N/A       NO    N/A    NO   0      NO      \n",
      "Education_Master                    N/A       NO    N/A    NO   0      NO      \n",
      "EducationField_Life Sciences        N/A       YES   N/A    NO   1      NO      \n",
      "EducationField_Marketing            N/A       NO    N/A    NO   0      NO      \n",
      "EducationField_Medical              N/A       NO    N/A    NO   0      NO      \n",
      "EducationField_Other                N/A       NO    N/A    NO   0      NO      \n",
      "EducationField_Technical Degree     N/A       NO    N/A    NO   0      NO      \n",
      "EnvironmentSatisfaction_Low         N/A       YES   N/A    YES  2      YES     \n",
      "EnvironmentSatisfaction_Medium      N/A       NO    N/A    NO   0      NO      \n",
      "... and 32 more features\n",
      "====================================================================================================\n",
      " SUMMARY\n",
      "Total features: 62\n",
      "Selected features: 18 (29.0%)\n",
      "Removed features: 44\n",
      "\n",
      " SELECTED FEATURES BY CATEGORY:\n",
      "  Numerical: 14 features\n",
      "    - Age (3 votes)\n",
      "    - DailyRate (2 votes)\n",
      "    - DistanceFromHome (2 votes)\n",
      "    - HourlyRate (2 votes)\n",
      "    - MonthlyIncome (3 votes)\n",
      "    - MonthlyRate (2 votes)\n",
      "    - NumCompaniesWorked (2 votes)\n",
      "    - PercentSalaryHike (2 votes)\n",
      "    - TotalWorkingYears (3 votes)\n",
      "    - TrainingTimesLastYear (2 votes)\n",
      "    - YearsAtCompany (3 votes)\n",
      "    - YearsInCurrentRole (3 votes)\n",
      "    - YearsSinceLastPromotion (2 votes)\n",
      "    - YearsWithCurrManager (3 votes)\n",
      "  Categorical (one-hot): 4 features\n",
      "    - BusinessTravel_Travel_Frequently (2 votes)\n",
      "    - EnvironmentSatisfaction_Low (2 votes)\n",
      "    - MaritalStatus_Single (2 votes)\n",
      "    - StockOptionLevel_nan (2 votes)\n",
      "\n",
      " VOTE DISTRIBUTION:\n",
      "  3 votes:  6 features (  9.7%)\n",
      "  2 votes: 12 features ( 19.4%)\n",
      "  1 votes: 11 features ( 17.7%)\n",
      "  0 votes: 33 features ( 53.2%)\n",
      "\n",
      " METHOD PERFORMANCE:\n",
      "  Spearman: 6/14 features selected (42.9%)\n",
      "  Chi2: 14/62 features selected (22.6%)\n",
      "  ANOVA: 14/14 features selected (100.0%)\n",
      "  RandomForest: 19/62 features selected (30.6%)\n",
      "\n",
      " FINAL DATASET: (1470, 18)\n",
      "   Samples: 1470\n",
      "   Features: 18\n",
      "   Reduction: 30 â†’ 18 features (40.0% reduction)\n"
     ]
    }
   ],
   "source": [
    "# Create a results table - use X_encoded_df columns instead of X.columns\n",
    "methods = ['Spearman', 'Chi2', 'ANOVA', 'RandomForest']\n",
    "results_table = pd.DataFrame(index=X_encoded_df.columns, columns=methods)\n",
    "\n",
    "# DEFINE chi2_selected based on our feature selection results\n",
    "chi2_selected = []\n",
    "for feature in X_encoded_df.columns:\n",
    "    if feature_votes[feature] >= 1:  # Features that got at least 1 vote from Chi2\n",
    "        # Check if this vote came from Chi2 (we need to track this)\n",
    "        # Since we used Chi2 on categorical_encoded_features, any categorical feature with votes likely came from Chi2\n",
    "        if any(cat in feature for cat in categorical_features):\n",
    "            chi2_selected.append(feature)\n",
    "\n",
    "print(f\"Chi2 selected {len(chi2_selected)} features\")\n",
    "\n",
    "# Fill in Spearman results (only for numeric features)\n",
    "for feature in X_encoded_df.columns:\n",
    "    if feature in numeric_features:  # Only numeric features for Spearman\n",
    "        corr, p_value = spearmanr(X_encoded_df[feature], y)\n",
    "        # Use the same threshold logic as before\n",
    "        if abs(corr) > 0.1 and p_value < 0.05:  # Your original threshold + significance\n",
    "            results_table.loc[feature, 'Spearman'] = 'YES'\n",
    "        else:\n",
    "            results_table.loc[feature, 'Spearman'] = 'NO'\n",
    "    else:\n",
    "        results_table.loc[feature, 'Spearman'] = 'N/A'\n",
    "\n",
    "# Fill in Chi2 results\n",
    "for feature in X_encoded_df.columns:\n",
    "    if feature in chi2_selected:\n",
    "        results_table.loc[feature, 'Chi2'] = 'YES'\n",
    "    else:\n",
    "        results_table.loc[feature, 'Chi2'] = 'NO'\n",
    "\n",
    "# Fill in ANOVA results (only for numeric features)\n",
    "for feature in X_encoded_df.columns:\n",
    "    if feature in numeric_features:  # Only numeric features for ANOVA\n",
    "        if feature in anova_selected:\n",
    "            results_table.loc[feature, 'ANOVA'] = 'YES'\n",
    "        else:\n",
    "            results_table.loc[feature, 'ANOVA'] = 'NO'\n",
    "    else:\n",
    "        results_table.loc[feature, 'ANOVA'] = 'N/A'\n",
    "\n",
    "# Fill in RandomForest results\n",
    "rf_importance_threshold = np.percentile(rf.feature_importances_, 70)\n",
    "for i, feature in enumerate(X_encoded_df.columns):\n",
    "    if rf.feature_importances_[i] > rf_importance_threshold:\n",
    "        results_table.loc[feature, 'RandomForest'] = 'YES'\n",
    "    else:\n",
    "        results_table.loc[feature, 'RandomForest'] = 'NO'\n",
    "\n",
    "# Add vote count and final decision (only count valid votes, not N/A)\n",
    "def count_valid_votes(row):\n",
    "    count = 0\n",
    "    for method in methods:\n",
    "        if row[method] == 'YES':\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "results_table['Votes'] = results_table.apply(count_valid_votes, axis=1)\n",
    "results_table['Selected'] = results_table['Votes'].apply(\n",
    "    lambda x: 'YES' if x >= 2 else 'NO'  # Your original threshold\n",
    ")\n",
    "\n",
    "# Display the table (show only first 30 rows to avoid overwhelming output)\n",
    "print(\"FEATURE SELECTION RESULTS TABLE (After One-Hot Encoding)\")\n",
    "print(\"=\"*100)\n",
    "print(f\"{'Feature':<35} {'Spearman':<9} {'Chi2':<5} {'ANOVA':<6} {'RF':<4} {'Votes':<6} {'Selected':<8}\")\n",
    "print(\"-\"*100)\n",
    "\n",
    "# Show all features but in a more readable format\n",
    "for feature in results_table.index[:30]:  # Show first 30 features\n",
    "    row = results_table.loc[feature]\n",
    "    print(f\"{feature:<35} {row['Spearman']:<9} {row['Chi2']:<5} {row['ANOVA']:<6} {row['RandomForest']:<4} {row['Votes']:<6} {row['Selected']:<8}\")\n",
    "\n",
    "if len(results_table) > 30:\n",
    "    print(f\"... and {len(results_table) - 30} more features\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Summary\n",
    "selected_count = (results_table['Selected'] == 'YES').sum()\n",
    "total_features = len(results_table)\n",
    "print(f\" SUMMARY\")\n",
    "print(f\"Total features: {total_features}\")\n",
    "print(f\"Selected features: {selected_count} ({selected_count/total_features*100:.1f}%)\")\n",
    "print(f\"Removed features: {total_features - selected_count}\")\n",
    "\n",
    "# Show selected features by category\n",
    "print(f\"\\n SELECTED FEATURES BY CATEGORY:\")\n",
    "selected_features_list = list(results_table[results_table['Selected'] == 'YES'].index)\n",
    "\n",
    "# Categorize selected features\n",
    "numeric_selected = [f for f in selected_features_list if f in numeric_features]\n",
    "categorical_selected = [f for f in selected_features_list if f not in numeric_features and f != 'OverTime']\n",
    "binary_selected = [f for f in selected_features_list if f == 'OverTime']\n",
    "\n",
    "print(f\"  Numerical: {len(numeric_selected)} features\")\n",
    "for feature in numeric_selected:\n",
    "    votes = results_table.loc[feature, 'Votes']\n",
    "    print(f\"    - {feature} ({votes} votes)\")\n",
    "\n",
    "print(f\"  Categorical (one-hot): {len(categorical_selected)} features\")\n",
    "# Show first 10 categorical features to avoid too much output\n",
    "for feature in categorical_selected[:10]:\n",
    "    votes = results_table.loc[feature, 'Votes']\n",
    "    print(f\"    - {feature} ({votes} votes)\")\n",
    "if len(categorical_selected) > 10:\n",
    "    print(f\"    ... and {len(categorical_selected) - 10} more\")\n",
    "\n",
    "if binary_selected:\n",
    "    print(f\"  Binary: {len(binary_selected)} features\")\n",
    "    for feature in binary_selected:\n",
    "        votes = results_table.loc[feature, 'Votes']\n",
    "        print(f\"    - {feature} ({votes} votes)\")\n",
    "\n",
    "# Show vote distribution\n",
    "print(f\"\\n VOTE DISTRIBUTION:\")\n",
    "vote_distribution = results_table['Votes'].value_counts().sort_index(ascending=False)\n",
    "for votes, count in vote_distribution.items():\n",
    "    percentage = (count / total_features) * 100\n",
    "    print(f\"  {votes} votes: {count:2d} features ({percentage:5.1f}%)\")\n",
    "\n",
    "# Method performance\n",
    "print(f\"\\n METHOD PERFORMANCE:\")\n",
    "for method in methods:\n",
    "    selected_by_method = (results_table[method] == 'YES').sum()\n",
    "    total_applicable = (results_table[method] != 'N/A').sum()\n",
    "    if total_applicable > 0:\n",
    "        percentage = (selected_by_method / total_applicable) * 100\n",
    "        print(f\"  {method}: {selected_by_method}/{total_applicable} features selected ({percentage:.1f}%)\")\n",
    "\n",
    "# Create the final reduced dataset\n",
    "X_reduced = X_encoded_df[selected_features_list]\n",
    "print(f\"\\n FINAL DATASET: {X_reduced.shape}\")\n",
    "print(f\"   Samples: {X_reduced.shape[0]}\")\n",
    "print(f\"   Features: {X_reduced.shape[1]}\")\n",
    "print(f\"   Reduction: {len(X.columns)} â†’ {X_reduced.shape[1]} features ({((len(X.columns) - X_reduced.shape[1]) / len(X.columns) * 100):.1f}% reduction)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a364a17d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_all_data: (1470, 63)\n",
      "df_keep_data: (1470, 19)\n"
     ]
    }
   ],
   "source": [
    "# Create df_all_data (all features + target)\n",
    "df_all_data = X_encoded_df.copy()\n",
    "df_all_data['Attrition'] = y\n",
    "\n",
    "# Create df_keep_data (selected features + target)  \n",
    "df_keep_data = X_reduced.copy()\n",
    "df_keep_data['Attrition'] = y\n",
    "\n",
    "print(f\"df_all_data: {df_all_data.shape}\")\n",
    "print(f\"df_keep_data: {df_keep_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bcf0bb",
   "metadata": {},
   "source": [
    "## 4. Predictive Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69eb736d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TARGET VARIABLE DISTRIBUTION:\n",
      "Attrition = 0 (Stayed): 1233 (83.9%)\n",
      "Attrition = 1 (Left):   237 (16.1%)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check target variable distribution\n",
    "print(\"TARGET VARIABLE DISTRIBUTION:\")\n",
    "target_counts = y.value_counts()\n",
    "target_percent = y.value_counts(normalize=True) * 100\n",
    "\n",
    "print(f\"Attrition = 0 (Stayed): {target_counts[0]} ({target_percent[0]:.1f}%)\")\n",
    "print(f\"Attrition = 1 (Left):   {target_counts[1]} ({target_percent[1]:.1f}%)\")\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(x=y)\n",
    "plt.title('Target Variable Distribution - Attrition')\n",
    "plt.xlabel('Attrition (0=No, 1=Yes)')\n",
    "plt.ylabel('Count')\n",
    "for i, count in enumerate(target_counts):\n",
    "    plt.text(i, count + 10, f'{count}\\n({target_percent[i]:.1f}%)', ha='center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3096ad4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š TARGET DISTRIBUTION:\n",
      "Class 0: 1233 (83.9%)\n",
      "Class 1: 237 (16.1%)\n",
      "ðŸš€ COMPREHENSIVE MODEL COMPARISON WITH PROPER TRAIN/VAL/TEST SPLITS\n",
      "\n",
      "================================================================================\n",
      "ðŸ” TESTING ON ALL FEATURES DATASET\n",
      "================================================================================\n",
      "\n",
      "ðŸ§ª Testing KNN...\n",
      "ðŸ“Š DATA SPLITS:\n",
      "  Training: 882 samples (60.0%)\n",
      "  Validation: 294 samples (20.0%)\n",
      "  Test: 294 samples (20.0%)\n",
      "ðŸ”„ Applying SMOTE to training data...\n",
      "  After SMOTE - Class 0: 739, Class 1: 739\n",
      "ðŸ”„ Performing cross-validation...\n",
      "  Cross-validation F1: 0.680 +/- 0.041\n",
      "ðŸ”„ Training final model...\n",
      "\n",
      "ðŸ“Š FINAL RESULTS for KNeighborsClassifier:\n",
      "Training - Micro F1: 0.802, Macro F1: 0.794, Weighted F1: 0.794\n",
      "Training Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.60      0.75       739\n",
      "           1       0.72      1.00      0.83       739\n",
      "\n",
      "    accuracy                           0.80      1478\n",
      "   macro avg       0.86      0.80      0.79      1478\n",
      "weighted avg       0.86      0.80      0.79      1478\n",
      "\n",
      "Validation - Micro F1: 0.531, Macro F1: 0.489, Weighted F1: 0.588\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.49      0.63       247\n",
      "           1       0.22      0.77      0.34        47\n",
      "\n",
      "    accuracy                           0.53       294\n",
      "   macro avg       0.57      0.63      0.49       294\n",
      "weighted avg       0.80      0.53      0.59       294\n",
      "\n",
      "TEST - Micro F1: 0.500, Macro F1: 0.463, Weighted F1: 0.559\n",
      "TEST Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.45      0.60       247\n",
      "           1       0.21      0.74      0.32        47\n",
      "\n",
      "    accuracy                           0.50       294\n",
      "   macro avg       0.55      0.60      0.46       294\n",
      "weighted avg       0.79      0.50      0.56       294\n",
      "\n",
      "\n",
      "ðŸ§ª Testing DT...\n",
      "ðŸ“Š DATA SPLITS:\n",
      "  Training: 882 samples (60.0%)\n",
      "  Validation: 294 samples (20.0%)\n",
      "  Test: 294 samples (20.0%)\n",
      "ðŸ”„ Applying SMOTE to training data...\n",
      "  After SMOTE - Class 0: 739, Class 1: 739\n",
      "ðŸ”„ Performing cross-validation...\n",
      "  Cross-validation F1: 0.805 +/- 0.019\n",
      "ðŸ”„ Training final model...\n",
      "\n",
      "ðŸ“Š FINAL RESULTS for DecisionTreeClassifier:\n",
      "Training - Micro F1: 0.816, Macro F1: 0.814, Weighted F1: 0.814\n",
      "Training Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.91      0.83       739\n",
      "           1       0.89      0.72      0.80       739\n",
      "\n",
      "    accuracy                           0.82      1478\n",
      "   macro avg       0.83      0.82      0.81      1478\n",
      "weighted avg       0.83      0.82      0.81      1478\n",
      "\n",
      "Validation - Micro F1: 0.844, Macro F1: 0.660, Weighted F1: 0.830\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.94      0.91       247\n",
      "           1       0.52      0.34      0.41        47\n",
      "\n",
      "    accuracy                           0.84       294\n",
      "   macro avg       0.70      0.64      0.66       294\n",
      "weighted avg       0.82      0.84      0.83       294\n",
      "\n",
      "TEST - Micro F1: 0.847, Macro F1: 0.684, Weighted F1: 0.838\n",
      "TEST Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.93      0.91       247\n",
      "           1       0.53      0.40      0.46        47\n",
      "\n",
      "    accuracy                           0.85       294\n",
      "   macro avg       0.71      0.67      0.68       294\n",
      "weighted avg       0.83      0.85      0.84       294\n",
      "\n",
      "\n",
      "ðŸ§ª Testing NN...\n",
      "ðŸ“Š DATA SPLITS:\n",
      "  Training: 882 samples (60.0%)\n",
      "  Validation: 294 samples (20.0%)\n",
      "  Test: 294 samples (20.0%)\n",
      "ðŸ”„ Applying SMOTE to training data...\n",
      "  After SMOTE - Class 0: 739, Class 1: 739\n",
      "ðŸ”„ Performing cross-validation...\n",
      "  Cross-validation F1: 0.941 +/- 0.020\n",
      "ðŸ”„ Training final model...\n",
      "\n",
      "ðŸ“Š FINAL RESULTS for MLPClassifier:\n",
      "Training - Micro F1: 1.000, Macro F1: 1.000, Weighted F1: 1.000\n",
      "Training Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       739\n",
      "           1       1.00      1.00      1.00       739\n",
      "\n",
      "    accuracy                           1.00      1478\n",
      "   macro avg       1.00      1.00      1.00      1478\n",
      "weighted avg       1.00      1.00      1.00      1478\n",
      "\n",
      "Validation - Micro F1: 0.830, Macro F1: 0.678, Weighted F1: 0.828\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.90      0.90       247\n",
      "           1       0.47      0.45      0.46        47\n",
      "\n",
      "    accuracy                           0.83       294\n",
      "   macro avg       0.68      0.67      0.68       294\n",
      "weighted avg       0.83      0.83      0.83       294\n",
      "\n",
      "TEST - Micro F1: 0.850, Macro F1: 0.706, Weighted F1: 0.846\n",
      "TEST Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.92      0.91       247\n",
      "           1       0.54      0.47      0.50        47\n",
      "\n",
      "    accuracy                           0.85       294\n",
      "   macro avg       0.72      0.70      0.71       294\n",
      "weighted avg       0.84      0.85      0.85       294\n",
      "\n",
      "\n",
      "ðŸ§ª Testing SVM...\n",
      "ðŸ“Š DATA SPLITS:\n",
      "  Training: 882 samples (60.0%)\n",
      "  Validation: 294 samples (20.0%)\n",
      "  Test: 294 samples (20.0%)\n",
      "ðŸ”„ Applying SMOTE to training data...\n",
      "  After SMOTE - Class 0: 739, Class 1: 739\n",
      "ðŸ”„ Performing cross-validation...\n",
      "  Cross-validation F1: 0.940 +/- 0.019\n",
      "ðŸ”„ Training final model...\n",
      "\n",
      "ðŸ“Š FINAL RESULTS for SVC:\n",
      "Training - Micro F1: 0.987, Macro F1: 0.987, Weighted F1: 0.987\n",
      "Training Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       739\n",
      "           1       0.99      0.98      0.99       739\n",
      "\n",
      "    accuracy                           0.99      1478\n",
      "   macro avg       0.99      0.99      0.99      1478\n",
      "weighted avg       0.99      0.99      0.99      1478\n",
      "\n",
      "Validation - Micro F1: 0.874, Macro F1: 0.704, Weighted F1: 0.857\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.97      0.93       247\n",
      "           1       0.71      0.36      0.48        47\n",
      "\n",
      "    accuracy                           0.87       294\n",
      "   macro avg       0.80      0.67      0.70       294\n",
      "weighted avg       0.86      0.87      0.86       294\n",
      "\n",
      "TEST - Micro F1: 0.874, Macro F1: 0.717, Weighted F1: 0.861\n",
      "TEST Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.96      0.93       247\n",
      "           1       0.68      0.40      0.51        47\n",
      "\n",
      "    accuracy                           0.87       294\n",
      "   macro avg       0.79      0.68      0.72       294\n",
      "weighted avg       0.86      0.87      0.86       294\n",
      "\n",
      "\n",
      "ðŸ§ª Testing RF...\n",
      "ðŸ“Š DATA SPLITS:\n",
      "  Training: 882 samples (60.0%)\n",
      "  Validation: 294 samples (20.0%)\n",
      "  Test: 294 samples (20.0%)\n",
      "ðŸ”„ Applying SMOTE to training data...\n",
      "  After SMOTE - Class 0: 739, Class 1: 739\n",
      "ðŸ”„ Performing cross-validation...\n",
      "  Cross-validation F1: 0.928 +/- 0.009\n",
      "ðŸ”„ Training final model...\n",
      "\n",
      "ðŸ“Š FINAL RESULTS for RandomForestClassifier:\n",
      "Training - Micro F1: 1.000, Macro F1: 1.000, Weighted F1: 1.000\n",
      "Training Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       739\n",
      "           1       1.00      1.00      1.00       739\n",
      "\n",
      "    accuracy                           1.00      1478\n",
      "   macro avg       1.00      1.00      1.00      1478\n",
      "weighted avg       1.00      1.00      1.00      1478\n",
      "\n",
      "Validation - Micro F1: 0.871, Macro F1: 0.647, Weighted F1: 0.838\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.99      0.93       247\n",
      "           1       0.85      0.23      0.37        47\n",
      "\n",
      "    accuracy                           0.87       294\n",
      "   macro avg       0.86      0.61      0.65       294\n",
      "weighted avg       0.87      0.87      0.84       294\n",
      "\n",
      "TEST - Micro F1: 0.864, Macro F1: 0.617, Weighted F1: 0.826\n",
      "TEST Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.99      0.92       247\n",
      "           1       0.82      0.19      0.31        47\n",
      "\n",
      "    accuracy                           0.86       294\n",
      "   macro avg       0.84      0.59      0.62       294\n",
      "weighted avg       0.86      0.86      0.83       294\n",
      "\n",
      "\n",
      "ðŸ§ª Testing LR...\n",
      "ðŸ“Š DATA SPLITS:\n",
      "  Training: 882 samples (60.0%)\n",
      "  Validation: 294 samples (20.0%)\n",
      "  Test: 294 samples (20.0%)\n",
      "ðŸ”„ Applying SMOTE to training data...\n",
      "  After SMOTE - Class 0: 739, Class 1: 739\n",
      "ðŸ”„ Performing cross-validation...\n",
      "  Cross-validation F1: 0.833 +/- 0.030\n",
      "ðŸ”„ Training final model...\n",
      "\n",
      "ðŸ“Š FINAL RESULTS for LogisticRegression:\n",
      "Training - Micro F1: 0.855, Macro F1: 0.854, Weighted F1: 0.854\n",
      "Training Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.83      0.85       739\n",
      "           1       0.84      0.88      0.86       739\n",
      "\n",
      "    accuracy                           0.85      1478\n",
      "   macro avg       0.86      0.85      0.85      1478\n",
      "weighted avg       0.86      0.85      0.85      1478\n",
      "\n",
      "Validation - Micro F1: 0.776, Macro F1: 0.667, Weighted F1: 0.796\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.80      0.86       247\n",
      "           1       0.38      0.64      0.48        47\n",
      "\n",
      "    accuracy                           0.78       294\n",
      "   macro avg       0.65      0.72      0.67       294\n",
      "weighted avg       0.83      0.78      0.80       294\n",
      "\n",
      "TEST - Micro F1: 0.782, Macro F1: 0.669, Weighted F1: 0.801\n",
      "TEST Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.81      0.86       247\n",
      "           1       0.39      0.62      0.48        47\n",
      "\n",
      "    accuracy                           0.78       294\n",
      "   macro avg       0.65      0.72      0.67       294\n",
      "weighted avg       0.83      0.78      0.80       294\n",
      "\n",
      "\n",
      "ðŸ§ª Testing XGBoost...\n",
      "ðŸ“Š DATA SPLITS:\n",
      "  Training: 882 samples (60.0%)\n",
      "  Validation: 294 samples (20.0%)\n",
      "  Test: 294 samples (20.0%)\n",
      "ðŸ”„ Applying SMOTE to training data...\n",
      "  After SMOTE - Class 0: 739, Class 1: 739\n",
      "ðŸ”„ Performing cross-validation...\n",
      "  Cross-validation F1: 0.921 +/- 0.011\n",
      "ðŸ”„ Training final model...\n",
      "\n",
      "ðŸ“Š FINAL RESULTS for XGBClassifier:\n",
      "Training - Micro F1: 1.000, Macro F1: 1.000, Weighted F1: 1.000\n",
      "Training Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       739\n",
      "           1       1.00      1.00      1.00       739\n",
      "\n",
      "    accuracy                           1.00      1478\n",
      "   macro avg       1.00      1.00      1.00      1478\n",
      "weighted avg       1.00      1.00      1.00      1478\n",
      "\n",
      "Validation - Micro F1: 0.864, Macro F1: 0.722, Weighted F1: 0.857\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.94      0.92       247\n",
      "           1       0.59      0.47      0.52        47\n",
      "\n",
      "    accuracy                           0.86       294\n",
      "   macro avg       0.75      0.70      0.72       294\n",
      "weighted avg       0.85      0.86      0.86       294\n",
      "\n",
      "TEST - Micro F1: 0.847, Macro F1: 0.671, Weighted F1: 0.835\n",
      "TEST Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.94      0.91       247\n",
      "           1       0.53      0.36      0.43        47\n",
      "\n",
      "    accuracy                           0.85       294\n",
      "   macro avg       0.71      0.65      0.67       294\n",
      "weighted avg       0.83      0.85      0.83       294\n",
      "\n",
      "\n",
      "ðŸ§ª Testing LGBM...\n",
      "ðŸ“Š DATA SPLITS:\n",
      "  Training: 882 samples (60.0%)\n",
      "  Validation: 294 samples (20.0%)\n",
      "  Test: 294 samples (20.0%)\n",
      "ðŸ”„ Applying SMOTE to training data...\n",
      "  After SMOTE - Class 0: 739, Class 1: 739\n",
      "ðŸ”„ Performing cross-validation...\n",
      "[LightGBM] [Info] Number of positive: 591, number of negative: 591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001104 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6044\n",
      "[LightGBM] [Info] Number of data points in the train set: 1182, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\carla\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\carla\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 591, number of negative: 591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000773 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5864\n",
      "[LightGBM] [Info] Number of data points in the train set: 1182, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 591, number of negative: 591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000681 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6022\n",
      "[LightGBM] [Info] Number of data points in the train set: 1182, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\carla\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\carla\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 591, number of negative: 592\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000875 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5840\n",
      "[LightGBM] [Info] Number of data points in the train set: 1183, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 592, number of negative: 591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000868 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5837\n",
      "[LightGBM] [Info] Number of data points in the train set: 1183, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\carla\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\carla\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\carla\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\carla\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Cross-validation F1: 0.923 +/- 0.008\n",
      "ðŸ”„ Training final model...\n",
      "[LightGBM] [Info] Number of positive: 739, number of negative: 739\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001234 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7846\n",
      "[LightGBM] [Info] Number of data points in the train set: 1478, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "\n",
      "ðŸ“Š FINAL RESULTS for LGBMClassifier:\n",
      "Training - Micro F1: 1.000, Macro F1: 1.000, Weighted F1: 1.000\n",
      "Training Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       739\n",
      "           1       1.00      1.00      1.00       739\n",
      "\n",
      "    accuracy                           1.00      1478\n",
      "   macro avg       1.00      1.00      1.00      1478\n",
      "weighted avg       1.00      1.00      1.00      1478\n",
      "\n",
      "Validation - Micro F1: 0.878, Macro F1: 0.722, Weighted F1: 0.863\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.97      0.93       247\n",
      "           1       0.70      0.40      0.51        47\n",
      "\n",
      "    accuracy                           0.88       294\n",
      "   macro avg       0.80      0.69      0.72       294\n",
      "weighted avg       0.86      0.88      0.86       294\n",
      "\n",
      "TEST - Micro F1: 0.857, Macro F1: 0.621, Weighted F1: 0.825\n",
      "TEST Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.98      0.92       247\n",
      "           1       0.67      0.21      0.32        47\n",
      "\n",
      "    accuracy                           0.86       294\n",
      "   macro avg       0.77      0.60      0.62       294\n",
      "weighted avg       0.84      0.86      0.82       294\n",
      "\n",
      "\n",
      "================================================================================\n",
      "ðŸ” TESTING ON SELECTED FEATURES DATASET\n",
      "================================================================================\n",
      "\n",
      "ðŸ§ª Testing KNN...\n",
      "ðŸ“Š DATA SPLITS:\n",
      "  Training: 882 samples (60.0%)\n",
      "  Validation: 294 samples (20.0%)\n",
      "  Test: 294 samples (20.0%)\n",
      "ðŸ”„ Applying SMOTE to training data...\n",
      "  After SMOTE - Class 0: 739, Class 1: 739\n",
      "ðŸ”„ Performing cross-validation...\n",
      "  Cross-validation F1: 0.824 +/- 0.012\n",
      "ðŸ”„ Training final model...\n",
      "\n",
      "ðŸ“Š FINAL RESULTS for KNeighborsClassifier:\n",
      "Training - Micro F1: 0.896, Macro F1: 0.895, Weighted F1: 0.895\n",
      "Training Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.81      0.89       739\n",
      "           1       0.84      0.99      0.90       739\n",
      "\n",
      "    accuracy                           0.90      1478\n",
      "   macro avg       0.91      0.90      0.89      1478\n",
      "weighted avg       0.91      0.90      0.89      1478\n",
      "\n",
      "Validation - Micro F1: 0.707, Macro F1: 0.593, Weighted F1: 0.740\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.74      0.81       247\n",
      "           1       0.29      0.55      0.38        47\n",
      "\n",
      "    accuracy                           0.71       294\n",
      "   macro avg       0.59      0.65      0.59       294\n",
      "weighted avg       0.80      0.71      0.74       294\n",
      "\n",
      "TEST - Micro F1: 0.687, Macro F1: 0.560, Weighted F1: 0.721\n",
      "TEST Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.73      0.80       247\n",
      "           1       0.25      0.47      0.32        47\n",
      "\n",
      "    accuracy                           0.69       294\n",
      "   macro avg       0.56      0.60      0.56       294\n",
      "weighted avg       0.78      0.69      0.72       294\n",
      "\n",
      "\n",
      "ðŸ§ª Testing DT...\n",
      "ðŸ“Š DATA SPLITS:\n",
      "  Training: 882 samples (60.0%)\n",
      "  Validation: 294 samples (20.0%)\n",
      "  Test: 294 samples (20.0%)\n",
      "ðŸ”„ Applying SMOTE to training data...\n",
      "  After SMOTE - Class 0: 739, Class 1: 739\n",
      "ðŸ”„ Performing cross-validation...\n",
      "  Cross-validation F1: 0.778 +/- 0.029\n",
      "ðŸ”„ Training final model...\n",
      "\n",
      "ðŸ“Š FINAL RESULTS for DecisionTreeClassifier:\n",
      "Training - Micro F1: 0.800, Macro F1: 0.799, Weighted F1: 0.799\n",
      "Training Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.87      0.81       739\n",
      "           1       0.85      0.73      0.79       739\n",
      "\n",
      "    accuracy                           0.80      1478\n",
      "   macro avg       0.81      0.80      0.80      1478\n",
      "weighted avg       0.81      0.80      0.80      1478\n",
      "\n",
      "Validation - Micro F1: 0.823, Macro F1: 0.646, Weighted F1: 0.816\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.91      0.90       247\n",
      "           1       0.44      0.36      0.40        47\n",
      "\n",
      "    accuracy                           0.82       294\n",
      "   macro avg       0.66      0.64      0.65       294\n",
      "weighted avg       0.81      0.82      0.82       294\n",
      "\n",
      "TEST - Micro F1: 0.779, Macro F1: 0.599, Weighted F1: 0.782\n",
      "TEST Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.86      0.87       247\n",
      "           1       0.32      0.34      0.33        47\n",
      "\n",
      "    accuracy                           0.78       294\n",
      "   macro avg       0.60      0.60      0.60       294\n",
      "weighted avg       0.78      0.78      0.78       294\n",
      "\n",
      "\n",
      "ðŸ§ª Testing NN...\n",
      "ðŸ“Š DATA SPLITS:\n",
      "  Training: 882 samples (60.0%)\n",
      "  Validation: 294 samples (20.0%)\n",
      "  Test: 294 samples (20.0%)\n",
      "ðŸ”„ Applying SMOTE to training data...\n",
      "  After SMOTE - Class 0: 739, Class 1: 739\n",
      "ðŸ”„ Performing cross-validation...\n",
      "  Cross-validation F1: 0.913 +/- 0.016\n",
      "ðŸ”„ Training final model...\n",
      "\n",
      "ðŸ“Š FINAL RESULTS for MLPClassifier:\n",
      "Training - Micro F1: 0.996, Macro F1: 0.996, Weighted F1: 0.996\n",
      "Training Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00       739\n",
      "           1       0.99      1.00      1.00       739\n",
      "\n",
      "    accuracy                           1.00      1478\n",
      "   macro avg       1.00      1.00      1.00      1478\n",
      "weighted avg       1.00      1.00      1.00      1478\n",
      "\n",
      "Validation - Micro F1: 0.779, Macro F1: 0.585, Weighted F1: 0.778\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.87      0.87       247\n",
      "           1       0.30      0.30      0.30        47\n",
      "\n",
      "    accuracy                           0.78       294\n",
      "   macro avg       0.59      0.58      0.58       294\n",
      "weighted avg       0.78      0.78      0.78       294\n",
      "\n",
      "TEST - Micro F1: 0.796, Macro F1: 0.583, Weighted F1: 0.786\n",
      "TEST Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.90      0.88       247\n",
      "           1       0.32      0.26      0.29        47\n",
      "\n",
      "    accuracy                           0.80       294\n",
      "   macro avg       0.59      0.58      0.58       294\n",
      "weighted avg       0.78      0.80      0.79       294\n",
      "\n",
      "\n",
      "ðŸ§ª Testing SVM...\n",
      "ðŸ“Š DATA SPLITS:\n",
      "  Training: 882 samples (60.0%)\n",
      "  Validation: 294 samples (20.0%)\n",
      "  Test: 294 samples (20.0%)\n",
      "ðŸ”„ Applying SMOTE to training data...\n",
      "  After SMOTE - Class 0: 739, Class 1: 739\n",
      "ðŸ”„ Performing cross-validation...\n",
      "  Cross-validation F1: 0.851 +/- 0.010\n",
      "ðŸ”„ Training final model...\n",
      "\n",
      "ðŸ“Š FINAL RESULTS for SVC:\n",
      "Training - Micro F1: 0.905, Macro F1: 0.905, Weighted F1: 0.905\n",
      "Training Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.92      0.91       739\n",
      "           1       0.92      0.89      0.90       739\n",
      "\n",
      "    accuracy                           0.91      1478\n",
      "   macro avg       0.91      0.91      0.91      1478\n",
      "weighted avg       0.91      0.91      0.91      1478\n",
      "\n",
      "Validation - Micro F1: 0.796, Macro F1: 0.627, Weighted F1: 0.798\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.87      0.88       247\n",
      "           1       0.37      0.38      0.38        47\n",
      "\n",
      "    accuracy                           0.80       294\n",
      "   macro avg       0.62      0.63      0.63       294\n",
      "weighted avg       0.80      0.80      0.80       294\n",
      "\n",
      "TEST - Micro F1: 0.782, Macro F1: 0.588, Weighted F1: 0.780\n",
      "TEST Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.87      0.87       247\n",
      "           1       0.31      0.30      0.30        47\n",
      "\n",
      "    accuracy                           0.78       294\n",
      "   macro avg       0.59      0.59      0.59       294\n",
      "weighted avg       0.78      0.78      0.78       294\n",
      "\n",
      "\n",
      "ðŸ§ª Testing RF...\n",
      "ðŸ“Š DATA SPLITS:\n",
      "  Training: 882 samples (60.0%)\n",
      "  Validation: 294 samples (20.0%)\n",
      "  Test: 294 samples (20.0%)\n",
      "ðŸ”„ Applying SMOTE to training data...\n",
      "  After SMOTE - Class 0: 739, Class 1: 739\n",
      "ðŸ”„ Performing cross-validation...\n",
      "  Cross-validation F1: 0.916 +/- 0.012\n",
      "ðŸ”„ Training final model...\n",
      "\n",
      "ðŸ“Š FINAL RESULTS for RandomForestClassifier:\n",
      "Training - Micro F1: 1.000, Macro F1: 1.000, Weighted F1: 1.000\n",
      "Training Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       739\n",
      "           1       1.00      1.00      1.00       739\n",
      "\n",
      "    accuracy                           1.00      1478\n",
      "   macro avg       1.00      1.00      1.00      1478\n",
      "weighted avg       1.00      1.00      1.00      1478\n",
      "\n",
      "Validation - Micro F1: 0.844, Macro F1: 0.652, Weighted F1: 0.828\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.94      0.91       247\n",
      "           1       0.52      0.32      0.39        47\n",
      "\n",
      "    accuracy                           0.84       294\n",
      "   macro avg       0.70      0.63      0.65       294\n",
      "weighted avg       0.82      0.84      0.83       294\n",
      "\n",
      "TEST - Micro F1: 0.827, Macro F1: 0.601, Weighted F1: 0.805\n",
      "TEST Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.94      0.90       247\n",
      "           1       0.42      0.23      0.30        47\n",
      "\n",
      "    accuracy                           0.83       294\n",
      "   macro avg       0.64      0.59      0.60       294\n",
      "weighted avg       0.79      0.83      0.81       294\n",
      "\n",
      "\n",
      "ðŸ§ª Testing LR...\n",
      "ðŸ“Š DATA SPLITS:\n",
      "  Training: 882 samples (60.0%)\n",
      "  Validation: 294 samples (20.0%)\n",
      "  Test: 294 samples (20.0%)\n",
      "ðŸ”„ Applying SMOTE to training data...\n",
      "  After SMOTE - Class 0: 739, Class 1: 739\n",
      "ðŸ”„ Performing cross-validation...\n",
      "  Cross-validation F1: 0.743 +/- 0.023\n",
      "ðŸ”„ Training final model...\n",
      "\n",
      "ðŸ“Š FINAL RESULTS for LogisticRegression:\n",
      "Training - Micro F1: 0.754, Macro F1: 0.754, Weighted F1: 0.754\n",
      "Training Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.73      0.75       739\n",
      "           1       0.74      0.78      0.76       739\n",
      "\n",
      "    accuracy                           0.75      1478\n",
      "   macro avg       0.76      0.75      0.75      1478\n",
      "weighted avg       0.76      0.75      0.75      1478\n",
      "\n",
      "Validation - Micro F1: 0.735, Macro F1: 0.631, Weighted F1: 0.764\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.75      0.83       247\n",
      "           1       0.33      0.64      0.43        47\n",
      "\n",
      "    accuracy                           0.73       294\n",
      "   macro avg       0.62      0.70      0.63       294\n",
      "weighted avg       0.82      0.73      0.76       294\n",
      "\n",
      "TEST - Micro F1: 0.694, Macro F1: 0.594, Weighted F1: 0.731\n",
      "TEST Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.71      0.80       247\n",
      "           1       0.29      0.62      0.39        47\n",
      "\n",
      "    accuracy                           0.69       294\n",
      "   macro avg       0.60      0.66      0.59       294\n",
      "weighted avg       0.81      0.69      0.73       294\n",
      "\n",
      "\n",
      "ðŸ§ª Testing XGBoost...\n",
      "ðŸ“Š DATA SPLITS:\n",
      "  Training: 882 samples (60.0%)\n",
      "  Validation: 294 samples (20.0%)\n",
      "  Test: 294 samples (20.0%)\n",
      "ðŸ”„ Applying SMOTE to training data...\n",
      "  After SMOTE - Class 0: 739, Class 1: 739\n",
      "ðŸ”„ Performing cross-validation...\n",
      "  Cross-validation F1: 0.900 +/- 0.006\n",
      "ðŸ”„ Training final model...\n",
      "\n",
      "ðŸ“Š FINAL RESULTS for XGBClassifier:\n",
      "Training - Micro F1: 1.000, Macro F1: 1.000, Weighted F1: 1.000\n",
      "Training Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       739\n",
      "           1       1.00      1.00      1.00       739\n",
      "\n",
      "    accuracy                           1.00      1478\n",
      "   macro avg       1.00      1.00      1.00      1478\n",
      "weighted avg       1.00      1.00      1.00      1478\n",
      "\n",
      "Validation - Micro F1: 0.830, Macro F1: 0.660, Weighted F1: 0.823\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.91      0.90       247\n",
      "           1       0.46      0.38      0.42        47\n",
      "\n",
      "    accuracy                           0.83       294\n",
      "   macro avg       0.67      0.65      0.66       294\n",
      "weighted avg       0.82      0.83      0.82       294\n",
      "\n",
      "TEST - Micro F1: 0.776, Macro F1: 0.575, Weighted F1: 0.774\n",
      "TEST Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.87      0.87       247\n",
      "           1       0.29      0.28      0.28        47\n",
      "\n",
      "    accuracy                           0.78       294\n",
      "   macro avg       0.58      0.57      0.57       294\n",
      "weighted avg       0.77      0.78      0.77       294\n",
      "\n",
      "\n",
      "ðŸ§ª Testing LGBM...\n",
      "ðŸ“Š DATA SPLITS:\n",
      "  Training: 882 samples (60.0%)\n",
      "  Validation: 294 samples (20.0%)\n",
      "  Test: 294 samples (20.0%)\n",
      "ðŸ”„ Applying SMOTE to training data...\n",
      "  After SMOTE - Class 0: 739, Class 1: 739\n",
      "ðŸ”„ Performing cross-validation...\n",
      "[LightGBM] [Info] Number of positive: 591, number of negative: 591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000187 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3857\n",
      "[LightGBM] [Info] Number of data points in the train set: 1182, number of used features: 18\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 591, number of negative: 591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000204 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1182, number of used features: 18\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 591, number of negative: 591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000213 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3849\n",
      "[LightGBM] [Info] Number of data points in the train set: 1182, number of used features: 18\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\carla\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\carla\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\carla\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 591, number of negative: 592\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000213 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3834\n",
      "[LightGBM] [Info] Number of data points in the train set: 1183, number of used features: 18\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 592, number of negative: 591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000282 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3849\n",
      "[LightGBM] [Info] Number of data points in the train set: 1183, number of used features: 18\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "  Cross-validation F1: 0.907 +/- 0.022\n",
      "ðŸ”„ Training final model...\n",
      "[LightGBM] [Info] Number of positive: 739, number of negative: 739\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000261 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4302\n",
      "[LightGBM] [Info] Number of data points in the train set: 1478, number of used features: 18\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\carla\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\carla\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\carla\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\carla\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\carla\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š FINAL RESULTS for LGBMClassifier:\n",
      "Training - Micro F1: 1.000, Macro F1: 1.000, Weighted F1: 1.000\n",
      "Training Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       739\n",
      "           1       1.00      1.00      1.00       739\n",
      "\n",
      "    accuracy                           1.00      1478\n",
      "   macro avg       1.00      1.00      1.00      1478\n",
      "weighted avg       1.00      1.00      1.00      1478\n",
      "\n",
      "Validation - Micro F1: 0.844, Macro F1: 0.636, Weighted F1: 0.823\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.95      0.91       247\n",
      "           1       0.52      0.28      0.36        47\n",
      "\n",
      "    accuracy                           0.84       294\n",
      "   macro avg       0.70      0.61      0.64       294\n",
      "weighted avg       0.82      0.84      0.82       294\n",
      "\n",
      "TEST - Micro F1: 0.820, Macro F1: 0.554, Weighted F1: 0.788\n",
      "TEST Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.95      0.90       247\n",
      "           1       0.35      0.15      0.21        47\n",
      "\n",
      "    accuracy                           0.82       294\n",
      "   macro avg       0.60      0.55      0.55       294\n",
      "weighted avg       0.77      0.82      0.79       294\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "ðŸ“Š FINAL COMPARISON SUMMARY\n",
      "====================================================================================================\n",
      "          Dataset   Model CV F1 Train F1 Val F1 TEST F1 Test Precision Test Recall Test Accuracy\n",
      "     All Features     KNN 0.680    0.794  0.588   0.559          0.792       0.500         0.500\n",
      "     All Features      DT 0.805    0.814  0.830   0.838          0.833       0.847         0.847\n",
      "     All Features      NN 0.941    1.000  0.828   0.846          0.843       0.850         0.850\n",
      "     All Features     SVM 0.940    0.987  0.857   0.861          0.860       0.874         0.874\n",
      "     All Features      RF 0.928    1.000  0.838   0.826          0.858       0.864         0.864\n",
      "     All Features      LR 0.833    0.854  0.796   0.801          0.833       0.782         0.782\n",
      "     All Features XGBoost 0.921    1.000  0.857   0.835          0.829       0.847         0.847\n",
      "     All Features    LGBM 0.923    1.000  0.863   0.825          0.835       0.857         0.857\n",
      "Selected Features     KNN 0.824    0.895  0.740   0.721          0.777       0.687         0.687\n",
      "Selected Features      DT 0.778    0.799  0.816   0.782          0.785       0.779         0.779\n",
      "Selected Features      NN 0.913    0.996  0.778   0.786          0.778       0.796         0.796\n",
      "Selected Features     SVM 0.851    0.905  0.798   0.780          0.779       0.782         0.782\n",
      "Selected Features      RF 0.916    1.000  0.828   0.805          0.795       0.827         0.827\n",
      "Selected Features      LR 0.743    0.754  0.764   0.731          0.808       0.694         0.694\n",
      "Selected Features XGBoost 0.900    1.000  0.823   0.774          0.772       0.776         0.776\n",
      "Selected Features    LGBM 0.907    1.000  0.823   0.788          0.773       0.820         0.820\n"
     ]
    }
   ],
   "source": [
    "# Your datasets\n",
    "df_all_data = X_encoded_df.copy()\n",
    "df_all_data['Attrition'] = y\n",
    "df_keep_data = X_reduced.copy()  \n",
    "df_keep_data['Attrition'] = y\n",
    "\n",
    "print(\"ðŸ“Š TARGET DISTRIBUTION:\")\n",
    "print(f\"Class 0: {(y == 0).sum()} ({((y == 0).sum()/len(y))*100:.1f}%)\")\n",
    "print(f\"Class 1: {(y == 1).sum()} ({((y == 1).sum()/len(y))*100:.1f}%)\")\n",
    "\n",
    "def encode_data(dataset):\n",
    "    return dataset\n",
    "\n",
    "def score(y_true, y_pred, set_name=\"Validation\"):\n",
    "    micro_f1 = f1_score(y_true, y_pred, average='micro')\n",
    "    macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    weighted_f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    print(f'{set_name} - Micro F1: {micro_f1:.3f}, Macro F1: {macro_f1:.3f}, Weighted F1: {weighted_f1:.3f}')\n",
    "    print(f'{set_name} Classification Report:')\n",
    "    print(classification_report(y_true=y_true, y_pred=y_pred))\n",
    "\n",
    "def train_and_evaluate_model(data, model, test_size=0.2, val_size=0.25):\n",
    "    \"\"\"\n",
    "    Proper train/validation/test split with cross-validation on training data\n",
    "    \"\"\"\n",
    "    # 1. First split: Separate test set (20%)\n",
    "    X = data.drop(\"Attrition\", axis=1).copy()\n",
    "    y = data[\"Attrition\"].copy()\n",
    "    \n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # 2. Second split: Separate validation set from remaining data (25% of remaining = 20% of total)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp, y_temp, test_size=val_size, random_state=42, stratify=y_temp\n",
    "    )\n",
    "    \n",
    "    print(f\"ðŸ“Š DATA SPLITS:\")\n",
    "    print(f\"  Training: {X_train.shape[0]} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "    print(f\"  Validation: {X_val.shape[0]} samples ({X_val.shape[0]/len(X)*100:.1f}%)\") \n",
    "    print(f\"  Test: {X_test.shape[0]} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "    \n",
    "    # Encode data (already done, but for consistency)\n",
    "    X_train = encode_data(X_train)\n",
    "    X_val = encode_data(X_val)\n",
    "    X_test = encode_data(X_test)\n",
    "    \n",
    "    # Apply SMOTE to training data only\n",
    "    print(f\"ðŸ”„ Applying SMOTE to training data...\")\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "    print(f\"  After SMOTE - Class 0: {(y_train_balanced == 0).sum()}, Class 1: {(y_train_balanced == 1).sum()}\")\n",
    "    \n",
    "    # Scale the data using training data\n",
    "    scaler = MinMaxScaler().fit(X_train_balanced)\n",
    "    X_train_scaled = scaler.transform(X_train_balanced)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Cross-validation on training data\n",
    "    print(f\"ðŸ”„ Performing cross-validation...\")\n",
    "    skf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
    "    cv_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in skf.split(X_train_scaled, y_train_balanced):\n",
    "        X_cv_train, X_cv_val = X_train_scaled[train_idx], X_train_scaled[val_idx]\n",
    "        y_cv_train, y_cv_val = y_train_balanced[train_idx], y_train_balanced[val_idx]\n",
    "        \n",
    "        model_cv = model.__class__(**model.get_params())\n",
    "        model_cv.fit(X_cv_train, y_cv_train)\n",
    "        y_cv_pred = model_cv.predict(X_cv_val)\n",
    "        cv_scores.append(f1_score(y_cv_val, y_cv_pred, average='weighted'))\n",
    "    \n",
    "    cv_f1 = np.mean(cv_scores)\n",
    "    print(f\"  Cross-validation F1: {cv_f1:.3f} +/- {np.std(cv_scores):.3f}\")\n",
    "    \n",
    "    # Train final model on full training data\n",
    "    print(f\"ðŸ”„ Training final model...\")\n",
    "    model.fit(X_train_scaled, y_train_balanced)\n",
    "    \n",
    "    # Predictions on all sets\n",
    "    y_train_pred = model.predict(X_train_scaled)\n",
    "    y_val_pred = model.predict(X_val_scaled)\n",
    "    y_test_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_metrics = [\n",
    "        f1_score(y_train_balanced, y_train_pred, average='weighted'),\n",
    "        precision_score(y_train_balanced, y_train_pred, average='weighted'),\n",
    "        recall_score(y_train_balanced, y_train_pred, average='weighted'),\n",
    "        accuracy_score(y_train_balanced, y_train_pred),\n",
    "    ]\n",
    "    \n",
    "    val_metrics = [\n",
    "        f1_score(y_val, y_val_pred, average='weighted'),\n",
    "        precision_score(y_val, y_val_pred, average='weighted'),\n",
    "        recall_score(y_val, y_val_pred, average='weighted'),\n",
    "        accuracy_score(y_val, y_val_pred),\n",
    "    ]\n",
    "    \n",
    "    test_metrics = [\n",
    "        f1_score(y_test, y_test_pred, average='weighted'),\n",
    "        precision_score(y_test, y_test_pred, average='weighted'),\n",
    "        recall_score(y_test, y_test_pred, average='weighted'),\n",
    "        accuracy_score(y_test, y_test_pred),\n",
    "    ]\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nðŸ“Š FINAL RESULTS for {model.__class__.__name__}:\")\n",
    "    score(y_train_balanced, y_train_pred, \"Training\")\n",
    "    score(y_val, y_val_pred, \"Validation\")\n",
    "    score(y_test, y_test_pred, \"TEST\")\n",
    "    \n",
    "    return {\n",
    "        'cv_f1': cv_f1,\n",
    "        'train_metrics': train_metrics,\n",
    "        'val_metrics': val_metrics,\n",
    "        'test_metrics': test_metrics,\n",
    "        'model': model,\n",
    "        'scaler': scaler\n",
    "    }\n",
    "\n",
    "def compare_datasets(models_dict, dataset_name, data):\n",
    "    \"\"\"\n",
    "    Compare models on a specific dataset\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"ðŸ” TESTING ON {dataset_name.upper()} DATASET\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for model_name, model in models_dict.items():\n",
    "        print(f\"\\nðŸ§ª Testing {model_name}...\")\n",
    "        result = train_and_evaluate_model(data, model)\n",
    "        results[model_name] = result\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Define models with proper parameters for imbalanced data\n",
    "models = {\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'DT': DecisionTreeClassifier(max_depth=3, random_state=42, class_weight='balanced'),\n",
    "    'NN': MLPClassifier(max_iter=2000, random_state=42),\n",
    "    'SVM': SVC(random_state=42, class_weight='balanced', probability=True),\n",
    "    'RF': RandomForestClassifier(random_state=42, class_weight='balanced'),\n",
    "    'LR': LogisticRegression(C=100, penalty=\"l2\", solver=\"lbfgs\", random_state=42, class_weight='balanced'),\n",
    "    'XGBoost': XGBClassifier(random_state=42, scale_pos_weight=(y == 0).sum()/(y == 1).sum()),\n",
    "    'LGBM': LGBMClassifier(random_state=42, class_weight='balanced')\n",
    "}\n",
    "\n",
    "# Compare both datasets\n",
    "print(\"ðŸš€ COMPREHENSIVE MODEL COMPARISON WITH PROPER TRAIN/VAL/TEST SPLITS\")\n",
    "\n",
    "all_data_results = compare_datasets(models, \"ALL FEATURES\", df_all_data)\n",
    "keep_data_results = compare_datasets(models, \"SELECTED FEATURES\", df_keep_data)\n",
    "\n",
    "# Create summary table\n",
    "print(f\"\\n{'='*100}\")\n",
    "print(f\"ðŸ“Š FINAL COMPARISON SUMMARY\")\n",
    "print(f\"{'='*100}\")\n",
    "\n",
    "summary_df = pd.DataFrame(\n",
    "    columns=[\n",
    "        \"Dataset\", \"Model\", \"CV F1\", \"Train F1\", \"Val F1\", \"TEST F1\", \n",
    "        \"Test Precision\", \"Test Recall\", \"Test Accuracy\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "row_idx = 0\n",
    "for dataset_name, results in [(\"All Features\", all_data_results), (\"Selected Features\", keep_data_results)]:\n",
    "    for model_name, result in results.items():\n",
    "        summary_df.loc[row_idx] = [\n",
    "            dataset_name,\n",
    "            model_name,\n",
    "            f\"{result['cv_f1']:.3f}\",\n",
    "            f\"{result['train_metrics'][0]:.3f}\",\n",
    "            f\"{result['val_metrics'][0]:.3f}\",\n",
    "            f\"{result['test_metrics'][0]:.3f}\",\n",
    "            f\"{result['test_metrics'][1]:.3f}\",\n",
    "            f\"{result['test_metrics'][2]:.3f}\",\n",
    "            f\"{result['test_metrics'][3]:.3f}\"\n",
    "        ]\n",
    "        row_idx += 1\n",
    "\n",
    "print(summary_df.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
